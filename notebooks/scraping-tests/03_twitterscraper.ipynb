{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitterscraper\n",
    "> Author: [Dawn Graham](https://dawngraham.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use twitterscraper to get historical tweets.\n",
    "\n",
    "Documentation:\n",
    "- https://pypi.org/project/twitterscraper/0.2.7/\n",
    "- https://github.com/taspinar/twitterscraper\n",
    "\n",
    "Tweepy API is used to append user location. (http://www.tweepy.org/)\n",
    "\n",
    "Versions used:\n",
    "- Python 3.6.6\n",
    "- pandas 0.23.4\n",
    "- tweepy 3.7.0\n",
    "- twitterscraper 0.9.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Tweepy API\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "\n",
    "# For Twitterscraper\n",
    "from twitterscraper import query_tweets\n",
    "\n",
    "# For dataframes\n",
    "import datetime as dt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate Tweepy API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time you execute the notebook, add all credentials so that you can save them in the pkl file, then you can remove the secret keys from the notebook because they will just be loaded from the pkl file.\n",
    "\n",
    "The pkl file contains sensitive information that can be used to take control of your twitter acccount, do not share it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dawn Graham\n"
     ]
    }
   ],
   "source": [
    "# Enter Twitter API info the first time running this notebook, then delete.\n",
    "# Credentials will be saved into and loaded from separate pkl file.\n",
    "if not os.path.exists('secret_twitter_credentials.pkl'):\n",
    "    Twitter={}\n",
    "    Twitter['Consumer Key'] = ''\n",
    "    Twitter['Consumer Secret'] = ''\n",
    "    Twitter['Access Token'] = ''\n",
    "    Twitter['Access Token Secret'] = ''\n",
    "    with open('secret_twitter_credentials.pkl','wb') as f:\n",
    "        pickle.dump(Twitter, f)\n",
    "else:\n",
    "    Twitter=pickle.load(open('secret_twitter_credentials.pkl','rb'))\n",
    "\n",
    "auth = OAuthHandler(Twitter['Consumer Key'], Twitter['Consumer Secret'])\n",
    "auth.set_access_token(Twitter['Access Token'], Twitter['Access Token Secret'])\n",
    "\n",
    "api = API(auth)\n",
    "\n",
    "# If the authentication was successful, you should\n",
    "# see the name of the account print out\n",
    "print(api.me().name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up dictionary and function to collect tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(query, begin, end):\n",
    "    \n",
    "    # Set up dictionary to collect tweets\n",
    "    tweets_dict = {'timestamp':[],\n",
    "                   'id':[],\n",
    "                   'text':[],\n",
    "                   'user':[],\n",
    "                   'user_location':[],\n",
    "                   'likes':[],\n",
    "                   'replies':[],\n",
    "                   'retweets':[],\n",
    "                   'query':[]\n",
    "                  }\n",
    "    \n",
    "    for tweet in query_tweets(query, begindate=begin, enddate=end, limit=10):\n",
    "        # Append info to tweets_dict\n",
    "        tweets_dict['timestamp'].append(tweet.timestamp)\n",
    "        tweets_dict['id'].append(tweet.id)\n",
    "        tweets_dict['text'].append(tweet.text)\n",
    "        tweets_dict['user'].append(tweet.user)\n",
    "        tweets_dict['user_location'].append(api.get_user(tweet.user).location) # Get with Tweepy API\n",
    "        tweets_dict['likes'].append(tweet.likes)\n",
    "        tweets_dict['replies'].append(tweet.replies)\n",
    "        tweets_dict['retweets'].append(tweet.retweets)\n",
    "        tweets_dict['query'].append(query)\n",
    "        \n",
    "    tweets = pd.DataFrame(tweets_dict)\n",
    "    tweets.set_index('timestamp', inplace=True)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and run desired query\n",
    "Compile query here: https://twitter.com/search-advanced  \n",
    "\n",
    "Notes:\n",
    "- Then end date is NOT included in search.\n",
    "- This seems to limit to 20 per day or 400 per search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter desired values for query\n",
    "query = 'power outage'\n",
    "\n",
    "# Begin and end dates - (yyyy, m, d)\n",
    "begin = dt.date(2010, 1, 1)\n",
    "end = dt.date(2010, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['power outage since:2010-01-01 until:2010-01-02']\n",
      "INFO: Querying power outage since:2010-01-01 until:2010-01-02\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2010-01-01%20until%3A2010-01-02.\n",
      "INFO: Got 0 tweets (0 new).\n"
     ]
    }
   ],
   "source": [
    "# Run query\n",
    "tweets = get_query(query, begin, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>user_location</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, text, user, user_location, likes, replies, retweets, query]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append results to scrapedtweets.csv\n",
    "with open('./data/scrapedtweets.csv', 'a') as f:\n",
    "    tweets.to_csv(f, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get URL of specific tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to return URL\n",
    "def get_url(timestamp):\n",
    "    user = tweets[timestamp]['user'][0]\n",
    "    tweet_id = tweets[timestamp]['id'][0]\n",
    "    print(f'https://twitter.com/{user}/status/{tweet_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Request URL by timestamp\n",
    "# get_url('2010-01-01 23:48:44')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Scraper with for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- To run this function, we can't get 'user location' with Tweepy due to Twitter limitations. If we want that info later, we could possibly do a separate function.\n",
    "- Query by location is only available for September 2010 or later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dictionary to collect tweets\n",
    "tweets_dict = {'timestamp':[],\n",
    "               'id':[],\n",
    "               'text':[],\n",
    "               'user':[],\n",
    "               'likes':[],\n",
    "               'replies':[],\n",
    "               'retweets':[],\n",
    "               'query':[]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_by_month(query, y, m, limit=None):\n",
    "    \n",
    "    # Get number of days in each the month\n",
    "    # Check for leap years\n",
    "    if (y%4==0 and y%100!=0 or y%400==0) & (m == 2):\n",
    "        total_d = 29\n",
    "    elif m == 2:\n",
    "        total_d = 28\n",
    "    elif m == [4, 6, 9, 11]:\n",
    "        total_d = 30\n",
    "    else:\n",
    "        total_d = 31\n",
    "    \n",
    "    # Set first start & end day\n",
    "    d = 1\n",
    "    end_d = d + 1\n",
    "    \n",
    "    # Run for number of days in month\n",
    "    for day in range(total_d):\n",
    "        \n",
    "        # Set search begin date\n",
    "        begin = dt.date(y, m, d)\n",
    "        \n",
    "        # Set search end date\n",
    "        # Enables setting to 1st day of next month to get results from last day of search month\n",
    "        if (end_d > total_d) & (m == 12):\n",
    "            end = dt.date(y+1, 1, 1)\n",
    "        elif end_d > total_d:\n",
    "            end = dt.date(y, m+1, 1)\n",
    "        else:\n",
    "            end = dt.date(y, m, end_d)\n",
    "        \n",
    "        # Run twitterscraper query\n",
    "        for tweet in query_tweets(query, begindate=begin, enddate=end, limit=limit):\n",
    "            # Append info to tweets_dict\n",
    "            tweets_dict['timestamp'].append(tweet.timestamp)\n",
    "            tweets_dict['id'].append(tweet.id)\n",
    "            tweets_dict['text'].append(tweet.text)\n",
    "            tweets_dict['user'].append(tweet.user)\n",
    "            tweets_dict['likes'].append(tweet.likes)\n",
    "            tweets_dict['replies'].append(tweet.replies)\n",
    "            tweets_dict['retweets'].append(tweet.retweets)\n",
    "            tweets_dict['query'].append(query)\n",
    "            \n",
    "        # Pause\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Increase begin and end search date by 1\n",
    "        d += 1\n",
    "        end_d += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['power outage since:2013-07-01 until:2013-07-02']\n",
      "INFO: Querying power outage since:2013-07-01 until:2013-07-02\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-01%20until%3A2013-07-02.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-02 until:2013-07-03']\n",
      "INFO: Querying power outage since:2013-07-02 until:2013-07-03\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-02%20until%3A2013-07-03.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-03 until:2013-07-04']\n",
      "INFO: Querying power outage since:2013-07-03 until:2013-07-04\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-03%20until%3A2013-07-04.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-04 until:2013-07-05']\n",
      "INFO: Querying power outage since:2013-07-04 until:2013-07-05\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-04%20until%3A2013-07-05.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-05 until:2013-07-06']\n",
      "INFO: Querying power outage since:2013-07-05 until:2013-07-06\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-05%20until%3A2013-07-06.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-06 until:2013-07-07']\n",
      "INFO: Querying power outage since:2013-07-06 until:2013-07-07\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-06%20until%3A2013-07-07.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-07 until:2013-07-08']\n",
      "INFO: Querying power outage since:2013-07-07 until:2013-07-08\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-07%20until%3A2013-07-08.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-08 until:2013-07-09']\n",
      "INFO: Querying power outage since:2013-07-08 until:2013-07-09\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-08%20until%3A2013-07-09.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-09 until:2013-07-10']\n",
      "INFO: Querying power outage since:2013-07-09 until:2013-07-10\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-09%20until%3A2013-07-10.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-10 until:2013-07-11']\n",
      "INFO: Querying power outage since:2013-07-10 until:2013-07-11\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-10%20until%3A2013-07-11.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-11 until:2013-07-12']\n",
      "INFO: Querying power outage since:2013-07-11 until:2013-07-12\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-11%20until%3A2013-07-12.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-12 until:2013-07-13']\n",
      "INFO: Querying power outage since:2013-07-12 until:2013-07-13\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-12%20until%3A2013-07-13.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-13 until:2013-07-14']\n",
      "INFO: Querying power outage since:2013-07-13 until:2013-07-14\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-13%20until%3A2013-07-14.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-14 until:2013-07-15']\n",
      "INFO: Querying power outage since:2013-07-14 until:2013-07-15\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-14%20until%3A2013-07-15.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-15 until:2013-07-16']\n",
      "INFO: Querying power outage since:2013-07-15 until:2013-07-16\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-15%20until%3A2013-07-16.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-16 until:2013-07-17']\n",
      "INFO: Querying power outage since:2013-07-16 until:2013-07-17\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-16%20until%3A2013-07-17.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-17 until:2013-07-18']\n",
      "INFO: Querying power outage since:2013-07-17 until:2013-07-18\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-17%20until%3A2013-07-18.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-18 until:2013-07-19']\n",
      "INFO: Querying power outage since:2013-07-18 until:2013-07-19\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-18%20until%3A2013-07-19.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-19 until:2013-07-20']\n",
      "INFO: Querying power outage since:2013-07-19 until:2013-07-20\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-19%20until%3A2013-07-20.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-20 until:2013-07-21']\n",
      "INFO: Querying power outage since:2013-07-20 until:2013-07-21\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-20%20until%3A2013-07-21.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-21 until:2013-07-22']\n",
      "INFO: Querying power outage since:2013-07-21 until:2013-07-22\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-21%20until%3A2013-07-22.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-22 until:2013-07-23']\n",
      "INFO: Querying power outage since:2013-07-22 until:2013-07-23\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-22%20until%3A2013-07-23.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-23 until:2013-07-24']\n",
      "INFO: Querying power outage since:2013-07-23 until:2013-07-24\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-23%20until%3A2013-07-24.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-24 until:2013-07-25']\n",
      "INFO: Querying power outage since:2013-07-24 until:2013-07-25\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-24%20until%3A2013-07-25.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-25 until:2013-07-26']\n",
      "INFO: Querying power outage since:2013-07-25 until:2013-07-26\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-25%20until%3A2013-07-26.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-26 until:2013-07-27']\n",
      "INFO: Querying power outage since:2013-07-26 until:2013-07-27\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-26%20until%3A2013-07-27.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-27 until:2013-07-28']\n",
      "INFO: Querying power outage since:2013-07-27 until:2013-07-28\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-27%20until%3A2013-07-28.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-28 until:2013-07-29']\n",
      "INFO: Querying power outage since:2013-07-28 until:2013-07-29\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-28%20until%3A2013-07-29.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-29 until:2013-07-30']\n",
      "INFO: Querying power outage since:2013-07-29 until:2013-07-30\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-29%20until%3A2013-07-30.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-30 until:2013-07-31']\n",
      "INFO: Querying power outage since:2013-07-30 until:2013-07-31\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-30%20until%3A2013-07-31.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: queries: ['power outage since:2013-07-31 until:2013-08-01']\n",
      "INFO: Querying power outage since:2013-07-31 until:2013-08-01\n",
      "INFO: Got 0 tweets for power%20outage%20since%3A2013-07-31%20until%3A2013-08-01.\n",
      "INFO: Got 0 tweets (0 new).\n"
     ]
    }
   ],
   "source": [
    "# Enter desired values for query\n",
    "query = 'power outage'\n",
    "year = 2013\n",
    "month = 7\n",
    "limit = 10\n",
    "\n",
    "# Run query\n",
    "tweets = query_by_month(query, year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 7)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.DataFrame(tweets_dict)\n",
    "tweets.set_index('timestamp', inplace=True)\n",
    "print(tweets.shape)\n",
    "print(tweets['id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get number of observations containing word\n",
    "# tweets['text'].str.contains('power', case=False, regex=True).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
